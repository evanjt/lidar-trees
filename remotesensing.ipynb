{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install some required libraries with conda\n",
    "!conda install -y -c conda-forge scikit-learn reportlab pdal geopandas pip jupyter geojson ipyvolume python-pdal rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import json\n",
    "import pdal\n",
    "import pandas as pd\n",
    "import os\n",
    "import ipyvolume.pylab as p3\n",
    "import matplotlib.cm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import rasterio\n",
    "import scipy\n",
    "import shapely\n",
    "import urllib.request\n",
    "\n",
    "from pyproj import Proj, transform\n",
    "from IPython.display import Image\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Disable pandas copy on slice warning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the local maximum of a point cloud according to radius and point density threshold.\n",
    "# Code from point cloud processing tutorial\n",
    "# https://github.com/rockestate/point-cloud-processing/blob/master/notebooks/point-cloud-processing.ipynb\n",
    "# More details in PCL\n",
    "# http://docs.pointclouds.org/trunk/classpcl_1_1_local_maximum.html\n",
    "\n",
    "def local_max(coords, radius, density_threshold=0):\n",
    "    '''\n",
    "    Find local maxima of points in a pointcloud.\n",
    "    '''\n",
    "    max_box = coords.copy()\n",
    "    for i in range(2):\n",
    "        max_box['X{}'.format(i)] = ((coords['X']/radius + i) /2).astype(int)*2\n",
    "        max_box['Y{}'.format(i)] = ((coords['Y']/radius + i) /2).astype(int)*2\n",
    "    max_box['X_'] = (coords['X']/radius).astype(int)\n",
    "    max_box['Y_'] = (coords['Y']/radius).astype(int)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            max_box[str(i)+str(j)] = max_box.groupby(['X{}'.format(i), 'Y{}'.format(j)])['Z'].transform(np.max)\n",
    "    density = max_box.groupby(['X_','Y_'])['Z'].transform(len)\n",
    "    is_max = (max_box['00'] == max_box['10']) & (max_box['10'] == max_box['01']) & (max_box['01'] == max_box['11']) & (max_box['11'] == coords['Z'])\n",
    "    \n",
    "    return coords[is_max & (density >= (density_threshold))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEARMAP_API_KEY = ''\n",
    "PLANET_API_KEY = ''\n",
    "LIDAR_FILENAME = os.path.join(os.getcwd(), 'RBG.las')\n",
    "OUT_FOLDER = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON of the PDAL pipeline\n",
    "pdal_pipeline = {\n",
    "    \"pipeline\": [ LIDAR_FILENAME,\n",
    "                 {   \"type\":\"filters.pmf\"},\n",
    "                 {   \"type\":\"filters.hag\"}\n",
    "                ]}\n",
    "pipeline = pdal.Pipeline(json.dumps(pdal_pipeline))\n",
    "pipeline.validate()\n",
    "point_count = pipeline.execute()\n",
    "lidar_df = pd.DataFrame(pipeline.arrays[0])\n",
    "print(\"Number of points in LiDAR:\", point_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correct XYZ values relative to 0 for matplotlib to display\n",
    "lidar_df['X_0'] = lidar_df['X']\n",
    "lidar_df['Y_0'] = lidar_df['Y']\n",
    "lidar_df['Z_0'] = lidar_df['Z']\n",
    "lidar_df['X'] = lidar_df['X'] - lidar_df['X_0'].min()\n",
    "lidar_df['Y'] = lidar_df['Y'] - lidar_df['Y_0'].min()\n",
    "lidar_df['Z'] = lidar_df['Z'] - lidar_df['Z_0'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classify ground, grass and trees to determined values\n",
    "grass_points = lidar_df.loc[(lidar_df['Classification'] == 1) & (lidar_df['HeightAboveGround'] < 0.7)]\n",
    "ground_points = lidar_df.loc[lidar_df['Classification'] == 2]\n",
    "tree_points = lidar_df.loc[(~lidar_df.index.isin(ground_points.index)) & (~lidar_df.index.isin(grass_points.index)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise LiDAR\n",
    "fig = p3.figure(width=1000)\n",
    "\n",
    "# Define all points and ground points for plot\n",
    "tree_points_plot = p3.scatter(tree_points['Y'].values, tree_points['Z'].values, tree_points['X'].values, color='brown', size=.2)\n",
    "ground_points_plot = p3.scatter(ground_points['Y'].values, ground_points['Z'].values, ground_points['X'].values, color='blue', size=.2)\n",
    "grass_points_plot = p3.scatter(grass_points['Y'].values, grass_points['Z'].values, grass_points['X'].values, color='green', size=.2)\n",
    "# Add labels to plot\n",
    "fig.xlabel='Y'\n",
    "fig.ylabel='Z'\n",
    "fig.zlabel='X'\n",
    "\n",
    "# Append both sets of points to plot\n",
    "fig.scatters.append(tree_points_plot)\n",
    "fig.scatters.append(ground_points_plot)\n",
    "fig.scatters.append(grass_points_plot)\n",
    "p3.squarelim()\n",
    "p3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off ground points, cluster trees with DBSCAN\n",
    "ground_points_plot.visible=False\n",
    "grass_points_plot.visible=False\n",
    "\n",
    "dbscan = DBSCAN(eps=1.2, min_samples=10).fit(tree_points[['X','Y','Z']].values)\n",
    "tree_points['tree_idx'] = dbscan.labels_\n",
    "\n",
    "print(\"Number of tree points:\",len(tree_points))\n",
    "print(\"Unique clusters:\",len(tree_points['tree_idx'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate colourmap for individual clusters\n",
    "# https://www.robotswillkillusall.org/posts/mpl-scatterplot-colorbar.html\n",
    "cmap = matplotlib.cm.get_cmap('flag')\n",
    "normalize = matplotlib.colors.Normalize(vmin=min(tree_points['tree_idx']), vmax=max(tree_points['tree_idx']))\n",
    "colors = [cmap(normalize(value)) for value in tree_points['tree_idx']]\n",
    "\n",
    "# Plot clusters of points back onto scatter plot\n",
    "classified_tree_points = p3.scatter(tree_points['Y'].values, tree_points['Z'].values, tree_points['X'].values, color=colors, size=.2)\n",
    "tree_points_plot.visible=False\n",
    "fig.scatters.append(classified_tree_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find treetops, then place a sphere on each one\n",
    "tree_tops = local_max(tree_points, radius=3, density_threshold=15)\n",
    "tree_top_spheres = p3.scatter(tree_tops['Y'].values, tree_tops['Z'].values, tree_tops['X'].values, color='black', size=5, marker='sphere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the tallest tree \n",
    "# This method assumes that all significant trees are the tallest in the LiDAR\n",
    "# Alternatively, the treetop closest to the centre tree could be used\n",
    "tree_top_spheres.visible = False\n",
    "tallest_tree = pd.DataFrame(tree_tops.loc[tree_tops['Z'].idxmax()]).T\n",
    "tallest_tree_sphere = p3.scatter(tallest_tree['Y'].values, tallest_tree['Z'].values, tallest_tree['X'].values, color='green', size=5, marker='sphere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tallest_tree_lidar_df = tree_points.loc[tree_points['tree_idx'] == 3]\n",
    "print(\"Number of points in tallest tree:\", len(tallest_tree_lidar_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only the tallest tree's LiDAR\n",
    "tallest_tree_lidar_points = p3.scatter(tallest_tree_lidar_df['Y'].values, tallest_tree_lidar_df['Z'].values, tallest_tree_lidar_df['X'].values, color='green', size=.2)\n",
    "classified_tree_points.visible=False\n",
    "fig.scatters.append(tallest_tree_lidar_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume and height of tree cloud (in cubic metres, expecting coordinate system is metres - UTM WGS84 in this case)\n",
    "tree_volume = round(scipy.spatial.ConvexHull(tallest_tree_lidar_df[['X','Y','Z']]).volume,2)\n",
    "tree_height = round(tallest_tree['HeightAboveGround'].values[0],2)\n",
    "print(\"Tree dimensions\\n---------------\\nVolume:\", str(tree_volume)+\"m^3\", \"\\nHeight:\", str(tree_height)+\"m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearmap imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Y Z values of tallest tree in GDA94\n",
    "tallest_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_xy(from_crs, to_crs, x, y):\n",
    "    inProj = Proj(init='epsg:'+str(from_crs))\n",
    "    outProj = Proj(init='epsg:'+str(to_crs))\n",
    "    \n",
    "    return transform(inProj,outProj,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject coordinates from LiDAR into latitude and longitude\n",
    "\n",
    "project_crs = 28355 # GDA94 Z55\n",
    "nearmap_crs = 4326 # WGS 84\n",
    "\n",
    "lon, lat = reproject_xy(project_crs, nearmap_crs, tallest_tree['X_0'].values[0], tallest_tree['Y_0'].values[0])\n",
    "\n",
    "print(\"Tree's lat/long:\", str(lat) + ', ' + str(lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect todays date for latest Nearmap imagery\n",
    "todays_date = datetime.datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# Form a Nearmap API query\n",
    "# API docs for imagery: https://docs.nearmap.com/display/ND/Image+API\n",
    "nearmap_url = \"http://au0.nearmap.com/staticmap?center=\"+str(y2)+\",\"\\\n",
    "                +str(x2)+\"&size=3000x3000&zoom=22&date=\"+todays_date\\\n",
    "                +\"&httpauth=false&apikey=\"+NEARMAP_API_KEY\n",
    "\n",
    "# Retrieve data and save as a local file\n",
    "# URL Retrieve https://docs.python.org/3.0/library/urllib.request.html\n",
    "nm_filename, nm_url = urllib.request.urlretrieve(nearmap_url, os.path.join(os.getcwd(), 'nearmap.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show nearmap image in notebooks\n",
    "Image(filename='nearmap.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planet multispectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convex hull around tree points, then create bounding box for Planet imagery acquisition\n",
    "tree_polygon = shapely.geometry.MultiPoint(tallest_tree_lidar_df[['X_0','Y_0']].values).convex_hull\n",
    "\n",
    "# Reproject coordinates to WGS84 (suitable for Planet)\n",
    "tree_bbox = [reproject_xy(project_crs, nearmap_crs, tree_polygon.bounds[0], tree_polygon.bounds[1]),\n",
    "            reproject_xy(project_crs, nearmap_crs, tree_polygon.bounds[2], tree_polygon.bounds[3])]\n",
    "tree_bbox_wgs84 = shapely.geometry.box(tree_bbox[0][0], tree_bbox[0][1], tree_bbox[1][0], tree_bbox[1][1])\n",
    "tree_bbox_list = list(bbox_wgs84.exterior.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been adapted from the Planet API resource notebooks at:\n",
    "# https://github.com/planetlabs/notebooks/blob/master/jupyter-notebooks/data-api-tutorials/planet_data_api_introduction.ipynb\n",
    "# as well as their NDVI method at:\n",
    "# https://developers.planet.com/tutorials/calculate-ndvi/\n",
    "\n",
    "# Helper function to printformatted JSON using the json module\n",
    "def p(data):\n",
    "    print(json.dumps(data, indent=2))\n",
    "    \n",
    "# Function to assist with downloading Planet data\n",
    "def pl_download(url, folder, api_key, filename=None):\n",
    "    # Send a GET request to the provided location url, using your API Key for authentication\n",
    "    res = requests.get(url, stream=True, auth=(api_key, \"\"))\n",
    "    # If no filename argument is given\n",
    "    if not filename:\n",
    "        # Construct a filename from the API response\n",
    "        if \"content-disposition\" in res.headers:\n",
    "            filename = res.headers[\"content-disposition\"].split(\"filename=\")[-1].strip(\"'\\\"\")\n",
    "        # Construct a filename from the location url\n",
    "        else:\n",
    "            filename = url.split(\"=\")[1][:10]\n",
    "            \n",
    "    if not os.path.isdir(os.path.join('.',folder)):\n",
    "        os.makedirs(os.path.join('.',folder))\n",
    "        print (\"Created folder\", os.path.join('.',folder))\n",
    "    if not os.path.isfile(os.path.join('.',folder,filename)):\n",
    "        print(\"Downloading...\")\n",
    "        with open(os.path.join('.',folder, filename), \"wb\") as f:\n",
    "            for chunk in res.iter_content(chunk_size=1024):\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        print(os.path.join('.',folder,filename), \"is downloaded\")\n",
    "        print(\"Creating NDVI and .png preview\")\n",
    "        form_ndvi(filename, folder)\n",
    "    else:\n",
    "        print(os.path.join('.',folder,filename),\"already exists\")\n",
    "   \n",
    "    return filename    \n",
    "\n",
    "# Takes the input features list and returns a list of date, id and cloud cover\n",
    "def concat_feature_list(features, bbox_polygon):\n",
    "    feature_list = []\n",
    "    for f in features:\n",
    "        # Build bbox with Shapely to check if it fits our project's geometry\n",
    "        feature_bbox = shapely.geometry.Polygon(f['geometry']['coordinates'][0])\n",
    "        date = f[\"properties\"]['acquired'].split(':')[0].split('T')[0].split('-')\n",
    "        feature_list.append([date[2], date[1], date[0], f['id'], \n",
    "                             f['properties']['cloud_cover'], f['_links']['assets'], f['properties']['sun_azimuth'], f['properties']['sun_elevation'], f['properties']['view_angle'], feature_bbox.contains(bbox_polygon)])\n",
    "    return feature_list\n",
    "\n",
    "def acquire_planet_data(feature_asset_url, OUTPUT_DIR, session, api_key):\n",
    "    print(\"Working on\", feature_asset_url)\n",
    "    \n",
    "    # Get the assets link for the item\n",
    "    assets_url = feature_asset_url\n",
    "    # Send a GET request to the assets url for the item (Get the list of available assets for the item)\n",
    "    res = session.get(assets_url)\n",
    "    # Assign a variable to the response\n",
    "    assets = res.json()\n",
    "    # We want the analytic datatype\n",
    "    analytic = assets[\"analytic\"]\n",
    "\n",
    "    # Setup the activation url for a particular asset (in this case the visual asset)\n",
    "    activation_url = analytic[\"_links\"][\"activate\"]\n",
    "\n",
    "    # Send a request to the activation url to activate the item\n",
    "    res = session.get(activation_url)\n",
    "\n",
    "    # Print the response from the activation request\n",
    "\n",
    "    if res.status_code == 202:\n",
    "        print(\"202 - The request has been accepted and the activation will begin shortly.\")\n",
    "    elif res.status_code == 204:\n",
    "        print(\"204 - The asset is already active and no further action is needed.\")\n",
    "    elif res.status_code == 401:\n",
    "        print(\"401 - The user does not have permissions to download this file.\")\n",
    "    else:\n",
    "        print(\"Undefined error.\")\n",
    "\n",
    "    asset_activated = False\n",
    "\n",
    "    while asset_activated == False:\n",
    "        # Send a request to the item's assets url\n",
    "        res = session.get(assets_url)\n",
    "\n",
    "        # Assign a variable to the item's assets url response\n",
    "        assets = res.json()\n",
    "\n",
    "        # Assign a variable to the visual asset from the response\n",
    "        analytic = assets[\"analytic\"]\n",
    "\n",
    "        asset_status = analytic[\"status\"]\n",
    "\n",
    "        # If asset is already active, we are done\n",
    "        if asset_status == 'active':\n",
    "            asset_activated = True\n",
    "            print(\"Asset is active and ready to download\")\n",
    "\n",
    "    # Assign a variable to the visual asset's location endpoint\n",
    "    location_url = analytic[\"location\"]\n",
    "    # Download the file from an activated asset's location url\n",
    "    filename = pl_download(location_url, OUTPUT_DIR, api_key)\n",
    "    \n",
    "def get_planet_data(OUTPUT_DIR, boundingbox_list, bbox_polygon, api_key):\n",
    "    print(\"\\n------- Obtaining Planet Data & Calculating NDVI -------\")\n",
    "    # Setup Planet Data API base URL\n",
    "    URL = \"https://api.planet.com/data/v1\"\n",
    "\n",
    "    # Setup the session\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Authenticate\n",
    "    session.auth = (api_key, \"\")\n",
    "\n",
    "    # Specify the sensors/satellites or \"item types\" to include in our results, \n",
    "    # date range, cloud cover and bounding box\n",
    "    item_types = [\"PSScene4Band\"]\n",
    "\n",
    "     # Set from 2 years from today\n",
    "    thisyear = datetime.datetime.today().strftime('%Y')\n",
    "    thismonth = datetime.datetime.today().strftime('%m')\n",
    "    thisday = datetime.datetime.today().strftime('%d')\n",
    "    thistime = 'T00:00:00Z'\n",
    "    planet_start = str(int(thisyear)-2)+'-'+thismonth+'-'+thisday+thistime\n",
    "    planet_end = thisyear+'-'+thismonth+'-'+thisday+thistime\n",
    "    \n",
    "    date_filter = {\n",
    "      \"type\": \"DateRangeFilter\",\n",
    "      \"field_name\": \"acquired\",\n",
    "      \"config\": {\n",
    "        \"gt\": planet_start,\n",
    "        \"lte\": planet_end\n",
    "      }\n",
    "    }\n",
    "\n",
    "    range_filter = {\n",
    "      \"type\": \"RangeFilter\",\n",
    "      \"field_name\": \"cloud_cover\",\n",
    "      \"config\": {\n",
    "        \"lt\": 0.1\n",
    "      }\n",
    "    }\n",
    "\n",
    "    RBG_geometry_filter = {\n",
    "      \"type\": \"GeometryFilter\",\n",
    "      \"field_name\": \"geometry\",\n",
    "      \"config\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          boundingbox_list\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Combine the filters together in the config item\n",
    "    and_filter = {\n",
    "        \"type\": \"AndFilter\",\n",
    "        \"config\": [range_filter, RBG_geometry_filter, date_filter]\n",
    "    }\n",
    "    \n",
    "    # Make a GET request to the Planet Data API\n",
    "    res = session.get(URL)\n",
    "    \n",
    "    # Setup the stats URL\n",
    "    stats_url = \"{}/stats\".format(URL)\n",
    "\n",
    "    # Construct the request.\n",
    "    request = {\n",
    "        \"item_types\" : item_types,\n",
    "        \"interval\" : \"month\",\n",
    "        \"filter\" : and_filter\n",
    "    }\n",
    "    # Send the POST request to the API stats endpoint\n",
    "    res = session.post(stats_url, json=request)\n",
    "    \n",
    "    # Setup the quick search endpoint url\n",
    "    quick_url = \"{}/quick-search\".format(URL)\n",
    "\n",
    "    request = {\n",
    "        \"item_types\" : item_types,\n",
    "        \"interval\" : \"month\",\n",
    "        \"filter\" : and_filter\n",
    "    }\n",
    "\n",
    "    # Send the POST request to the API quick search endpoint\n",
    "    res = session.post(quick_url, json=request)\n",
    "\n",
    "    # Assign the response to a variable\n",
    "    geojson = res.json()\n",
    "    features = geojson[\"features\"]\n",
    "\n",
    "    # Loop over all the features from the response\n",
    "    suitable_images = pd.DataFrame(concat_feature_list(features, bbox_polygon), \n",
    "                                   columns=['day', 'month', 'year', 'id', 'cloud', 'asset_url', 'sun_azimuth', 'sun_elevation', 'view_angle', 'fits_project'])\n",
    "    print(\"Features:\", len(features), \"Total features:\", len(suitable_images))\n",
    "\n",
    "    # If a page reaches its maximum, select the next page and concatenate results until all results are found\n",
    "    while len(features) == 250:\n",
    "        next_url = geojson[\"_links\"][\"_next\"]\n",
    "        res = session.get(next_url)\n",
    "        geojson = res.json()\n",
    "        features = geojson[\"features\"]\n",
    "        suitable_images = suitable_images.append(pd.DataFrame(concat_feature_list(features, bbox_polygon), \n",
    "                                                              columns=['day', 'month', 'year', 'id', 'cloud', 'asset_url', 'sun_azimuth', 'sun_elevation', 'view_angle', 'fits_project']))\n",
    "        print(\"Features:\", len(features), \"Total features:\", len(suitable_images))\n",
    "    print(\"Finished loading results\")\n",
    "    \n",
    "    # Reset the dataframe index after appending\n",
    "    suitable_images = suitable_images.reset_index()\n",
    "\n",
    "    # Clean out images that don't fit within the boundary of our project\n",
    "    #suitable_images = suitable_images.loc[suitable_images['fits_project'] == True]\n",
    "\n",
    "    # Find the day of each month over the data series, where the cloud cover is the least\n",
    "\n",
    "    df1 = suitable_images.loc[suitable_images.groupby(['year', 'month'])['cloud'].idxmin()].set_index(['year', 'month'])\n",
    "    df2 = suitable_images.loc[suitable_images.groupby(['year', 'month'])['cloud'].idxmax()].set_index(['year', 'month'])\n",
    "\n",
    "    unique_days = pd.concat([df1, df2], axis=1, keys=('min','max'))\n",
    "    unique_days.columns = unique_days.columns.map('_'.join)\n",
    "    \n",
    "    # Should be ready to download... Go!\n",
    "    for asset_url in unique_days['min_asset_url']:\n",
    "        acquire_planet_data(asset_url, OUTPUT_DIR, session, api_key)\n",
    "    \n",
    "    satellite_data = unique_days\n",
    "    print(\"satellite_data\")\n",
    "    print(satellite_data)\n",
    "    satellite_data['year'], satellite_data['month'] = satellite_data.index[0]\n",
    "    satellite_data = satellite_data[['year', 'month','min_day','min_cloud', 'min_sun_azimuth','min_sun_elevation','min_view_angle']]\n",
    "    satellite_data = satellite_data.rename(columns={'min_day': 'day', \n",
    "                                                    'min_cloud': 'cloud_cover',\n",
    "                                                    'min_sun_azimuth': 'sun_azimuth',\n",
    "                                                    'min_sun_elevation': 'sun_elevation',\n",
    "                                                    'min_view_angle': 'view_angle'})\n",
    "\n",
    "    return satellite_data\n",
    "\n",
    "def form_ndvi(filename, folder):\n",
    "     # The following NDVI calculation method is adapted from https://developers.planet.com/tutorials/calculate-ndvi/\n",
    "    with rasterio.open(os.path.join('.',folder,filename)) as src:\n",
    "        band_red = src.read(3)\n",
    "    with rasterio.open(os.path.join('.',folder,filename)) as src:\n",
    "        band_nir = src.read(4)\n",
    "\n",
    "    # Allow division by zero\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    # Calculate NDVI\n",
    "    ndvi = (band_nir.astype(float) - band_red.astype(float)) / (band_nir + band_red)\n",
    "\n",
    "    # Set spatial characteristics of the output object to mirror the input\n",
    "    kwargs = src.meta\n",
    "    kwargs.update(\n",
    "        dtype=rasterio.float32,\n",
    "        count = 1)\n",
    "\n",
    "    # Create the file\n",
    "    with rasterio.open(os.path.join('.',folder,filename+'.NDVI.tif'), 'w', **kwargs) as dst:\n",
    "            dst.write_band(1, ndvi.astype(rasterio.float32))\n",
    "\n",
    "    # Export as PNG \n",
    "    plt.imsave(os.path.join('.',folder,filename+'ndvi_cmap.png'), ndvi)\n",
    "    \n",
    "def crop_ndvi(shapefile, rasterin, rasterout, folder):\n",
    "    outfolder = os.path.join('.',folder,'ndvi')\n",
    "    \n",
    "    # Create NDVI folder\n",
    "    if not os.path.isdir(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "        print (\"Created folder\", outfolder)\n",
    "        \n",
    "    with fiona.open(shapefile, \"r\") as shapefile:\n",
    "        shpfeatures = [feature[\"geometry\"] for feature in shapefile]\n",
    "        \n",
    "    with rasterio.open(os.path.join('.',folder,rasterin), \"r\") as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shpfeatures, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "        \n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                 \"height\": out_image.shape[1],\n",
    "                 \"width\": out_image.shape[2],\n",
    "                 \"transform\": out_transform})\n",
    "    \n",
    "    with rasterio.open(os.path.join(outfolder,rasterout), \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "        \n",
    "def crop_the_planet(outdir):\n",
    "    print(\"\\n------- Cropping Planet Data -------\")\n",
    "    for tif in glob.iglob(os.path.join(os.getcwd(),outdir,'*NDVI.tif')):\n",
    "        outtif = os.path.join(tif.split('/')[-1:][0].split('_')[0][:-2] + '.ndvi.tif')\n",
    "        crop_ndvi(boundary_shapefile, tif, outtif, OUTPUT_DIR)\n",
    "        print(tif.split('/')[-1].split('_')[0][:-2], \"- Cropped NDVI:\", outtif)\n",
    "\n",
    "    for tif in glob.iglob(os.path.join(os.getcwd(),outdir,'ndvi','*.tif')):\n",
    "        with rasterio.open(tif) as src:\n",
    "            ndvi = src.read(1)\n",
    "        png_filename = os.path.join('.',outdir,'ndvi',tif.split('.')[0]+'ndvi_cmap.png')\n",
    "        plt.imsave(png_filename, ndvi)\n",
    "        print(tif.split('/')[-1].split('.')[0], \"- PNG file output:\", png_filename.split(' ')[-1].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Request planet data, then download\n",
    "get_planet_data(OUT_FOLDER, tree_bbox_list, tree_bbox_wgs84, PLANET_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the NDVI to the boundary of the RBG. This won't work without creating a shapefile\n",
    "# of the RBG boundary. \n",
    "\n",
    "#crop_the_planet(OUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* Be aware that Planet are changing their API rules from the 15th of May. Refer to their API documentation for updates https://developers.planet.com/docs/api/\n",
    "* Access to the original code can be found on my Github page. Any changes will require some editing. I cannot distribute the LiDAR data. The code was written very ad-hoc and is not illustrative of best practices. https://github.com/evanjt/lidar-trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "## Tutorial adapted from several tutorials and resources listed below\n",
    "\n",
    "* https://github.com/rockestate/point-cloud-processing/blob/master/notebooks/point-cloud-processing.ipynb\n",
    "* http://docs.pointclouds.org/trunk/classpcl_1_1_local_maximum.html\n",
    "* https://developers.planet.com/tutorials/calculate-ndvi/\n",
    "* https://github.com/planetlabs/notebooks/blob/master/jupyter-notebooks/data-api-tutorials/planet_data_api_introduction.ipynb\n",
    "* https://www.robotswillkillusall.org/posts/mpl-scatterplot-colorbar.html\n",
    "* Royal Botanic Gardens, Melbourne & MUASIP @ The University of Melbourne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
